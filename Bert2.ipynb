{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e157249-f737-447c-bf72-c7496b79c161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing and fine-tuning of BERT...\n",
      "Starting to parse XML files in 'spoken/tagged'\n",
      "Parsing file: S23A-tgd.xml...\n",
      "Parsing file: S24A-tgd.xml...\n",
      "Parsed 2 files. Stopping as per the file limit.\n",
      "Finished parsing files.\n",
      "Completed parsing and preprocessing. Splitting data into train and test sets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b606525d9116472c9d83bf8f6604a6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e941a1fa4d7419988f716e249210978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tokenization complete. Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nerfg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 1:34:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.185048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Evaluating model on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report saved to fine_tuned_bert_model/classification_report.txt\n",
      "Errors saved to fine_tuned_bert_model/errors.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def parse_bnc_folder(folder_path, file_limit=2):\n",
    "    data = []\n",
    "    file_count = 0\n",
    "    print(f\"Starting to parse XML files in '{folder_path}'\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            print(f\"Parsing file: {filename}...\")\n",
    "            tree = ET.parse(os.path.join(folder_path, filename))\n",
    "            root = tree.getroot()\n",
    "            for u in root.findall('u'):\n",
    "                sentence = []\n",
    "                labels = []\n",
    "                for w in u.findall('w'):\n",
    "                    sentence.append(w.text)\n",
    "                    labels.append(1 if w.attrib['pos'] in ['RR', 'UH'] else 0)\n",
    "                data.append((' '.join(sentence), labels))\n",
    "            file_count += 1\n",
    "            if file_count >= file_limit:\n",
    "                print(f\"Parsed {file_count} files. Stopping as per the file limit.\")\n",
    "                break  \n",
    "    print(\"Finished parsing files.\")\n",
    "    return pd.DataFrame(data, columns=['sentence', 'labels'])\n",
    "\n",
    "def preprocess_and_fine_tune_bert(folder_path, model_save_path):\n",
    "    print(\"Starting preprocessing and fine-tuning of BERT...\")\n",
    "    \n",
    "    df = parse_bnc_folder(folder_path, file_limit=2)\n",
    "    \n",
    "    df['labels'] = df['labels'].apply(lambda x: 1 if 1 in x else 0)\n",
    "\n",
    "    print(\"Completed parsing and preprocessing. Splitting data into train and test sets...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['sentence'], df['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "    train_data = Dataset.from_pandas(pd.DataFrame({'sentence': X_train, 'labels': y_train}))\n",
    "    test_data = Dataset.from_pandas(pd.DataFrame({'sentence': X_test, 'labels': y_test}))\n",
    "\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    print(\"Data tokenization complete. Loading BERT model...\")\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_save_path,          \n",
    "        evaluation_strategy=\"epoch\",        \n",
    "        per_device_train_batch_size=8,       \n",
    "        per_device_eval_batch_size=8,       \n",
    "        num_train_epochs=1,  # Reduced for testing\n",
    "        save_steps=10_000,                   \n",
    "        save_total_limit=2,                  \n",
    "        logging_dir='./logs',\n",
    "        fp16=True, \n",
    "        gradient_accumulation_steps=2,  #  larger batch size\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         \n",
    "        args=training_args,                  \n",
    "        train_dataset=train_data,           \n",
    "        eval_dataset=test_data               \n",
    "    )\n",
    "\n",
    "    print(\"Training BERT model...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"Training complete. Evaluating model on test set...\")\n",
    "    \n",
    "    predictions = trainer.predict(test_data)\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "    report = classification_report(y_test, preds)\n",
    "\n",
    "    with open(f'{model_save_path}/classification_report.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"Classification report saved to {model_save_path}/classification_report.txt\")\n",
    "\n",
    "    errors = []\n",
    "    for idx, (pred, true) in enumerate(zip(preds, y_test)):\n",
    "        if pred != true:\n",
    "            sentence = X_test.iloc[idx]\n",
    "            errors.append((sentence, true, pred))\n",
    "\n",
    "    with open(f'{model_save_path}/errors.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Sentence, True Label, Predicted Label\\n\")\n",
    "        for sentence, true_label, pred_label in errors:\n",
    "            f.write(f\"'{sentence}', {true_label}, {pred_label}\\n\")\n",
    "\n",
    "    print(f\"Errors saved to {model_save_path}/errors.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_and_fine_tune_bert('spoken/tagged', 'fine_tuned_bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe997337-6991-46ed-a1e8-e98a8b21a691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
